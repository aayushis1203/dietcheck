{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNk7505aKBSQKGCW+icH6Wd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aayushis1203/dietcheck/blob/main/00_data_collection_and_automatic_labels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Required Packages\n",
        "\n",
        "Installs all Python dependencies needed for data collection and processing."
      ],
      "metadata": {
        "id": "So_rL9DZeQGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install openfoodfacts pandas numpy scikit-learn matplotlib seaborn -q\n",
        "\n",
        "print(\"All packages installed successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plgdTUxbeN5n",
        "outputId": "ea1ddf28-1edb-4f42-f960-d0b4805b1679"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All packages installed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Import Core Libraries\n",
        "\n",
        "Imports all necessary Python libraries and configures visualization settings for consistent styling."
      ],
      "metadata": {
        "id": "xLhPjInFUACa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import core libraries\n",
        "import os\n",
        "import json\n",
        "import subprocess\n",
        "import re  # For robust serving size parsing\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure visualization style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"Libraries imported successfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRO7teGfT8fq",
        "outputId": "60733a7b-0609-4afe-9871-6ed343fffd5c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Setup Workspace and Directory Structure\n",
        "\n",
        "Handles environment detection (Colab vs local) and sets up the project directory structure.\n",
        "- **In Colab:** Clones GitHub repo if not already present\n",
        "- **Locally:** Finds repository root automatically\n",
        "- **Creates:** `data/` and `results/` directories\n"
      ],
      "metadata": {
        "id": "lxxuLon3UNrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CONFIGURATION - Update this with your GitHub repository URL\n",
        "# ============================================================================\n",
        "GITHUB_REPO = \"https://github.com/aayushis1203/dietcheck.git\"\n",
        "REPO_NAME = GITHUB_REPO.split('/')[-1].replace('.git', '')\n",
        "\n",
        "# ============================================================================\n",
        "# Helper Functions\n",
        "# ============================================================================\n",
        "\n",
        "def find_repo_root():\n",
        "    \"\"\"\n",
        "    Find repository root by searching for .git directory.\n",
        "    Prevents nested repo cloning if already inside repo.\n",
        "    \"\"\"\n",
        "    current = os.path.abspath(os.getcwd())\n",
        "\n",
        "    for _ in range(5):  # Search up to 5 levels\n",
        "        if os.path.exists(os.path.join(current, '.git')):\n",
        "            return current\n",
        "        parent = os.path.dirname(current)\n",
        "        if parent == current:\n",
        "            break\n",
        "        current = parent\n",
        "\n",
        "    return None\n",
        "\n",
        "def setup_workspace():\n",
        "    \"\"\"\n",
        "    Setup workspace for both Colab and local environments.\n",
        "    Returns absolute paths to repo root, data, and results directories.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import google.colab\n",
        "        in_colab = True\n",
        "        print(\"üîß Running in Google Colab\")\n",
        "\n",
        "        # Check if already inside repo (prevents nested cloning)\n",
        "        repo_root = find_repo_root()\n",
        "\n",
        "        if repo_root:\n",
        "            print(f\"‚úÖ Already inside repo at: {repo_root}\")\n",
        "            os.chdir(repo_root)\n",
        "        else:\n",
        "            # Clone repo if not present\n",
        "            if not os.path.exists(REPO_NAME):\n",
        "                print(f\"üì• Cloning {GITHUB_REPO}...\")\n",
        "                result = subprocess.run(\n",
        "                    ['git', 'clone', GITHUB_REPO],\n",
        "                    capture_output=True,\n",
        "                    text=True\n",
        "                )\n",
        "                if result.returncode != 0:\n",
        "                    raise RuntimeError(f\"Git clone failed: {result.stderr}\")\n",
        "\n",
        "            os.chdir(REPO_NAME)\n",
        "\n",
        "    except ImportError:\n",
        "        in_colab = False\n",
        "        print(\"üîß Running locally\")\n",
        "\n",
        "        # Find repo root automatically\n",
        "        repo_root = find_repo_root()\n",
        "\n",
        "        if repo_root:\n",
        "            os.chdir(repo_root)\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  Warning: Not in a git repository, using current directory\")\n",
        "\n",
        "    # Get absolute paths\n",
        "    repo_root = os.path.abspath(os.getcwd())\n",
        "    data_dir = os.path.join(repo_root, 'data')\n",
        "    results_dir = os.path.join(repo_root, 'results')\n",
        "\n",
        "    # Create directories\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"‚úÖ Repo root: {repo_root}\")\n",
        "    print(f\"üìÅ Data: {data_dir}\")\n",
        "    print(f\"üìÅ Results: {results_dir}\")\n",
        "\n",
        "    return repo_root, data_dir, results_dir\n",
        "\n",
        "# Execute setup and store paths\n",
        "REPO_ROOT, DATA_DIR, RESULTS_DIR = setup_workspace()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmaUdHc6US20",
        "outputId": "1f849dd4-1354-4f29-bb4c-9107da8c23bc"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Running in Google Colab\n",
            "‚úÖ Already inside repo at: /content/dietcheck\n",
            "‚úÖ Repo root: /content/dietcheck\n",
            "üìÅ Data: /content/dietcheck/data\n",
            "üìÅ Results: /content/dietcheck/results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Data Collection from Open Food Facts API\n",
        "\n",
        "Fetches products from multiple categories using requests with timeout handling."
      ],
      "metadata": {
        "id": "sTS_sIvoUoAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "from requests.adapters import HTTPAdapter\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "\n",
        "def create_robust_session():\n",
        "    session = requests.Session()\n",
        "    retry_strategy = Retry(\n",
        "        total=3,\n",
        "        backoff_factor=2,\n",
        "        status_forcelist=[429, 500, 502, 503, 504],\n",
        "    )\n",
        "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
        "    session.mount(\"http://\", adapter)\n",
        "    session.mount(\"https://\", adapter)\n",
        "    return session\n",
        "\n",
        "def fetch_products_from_category(category, page_size=15, max_products=25):\n",
        "    session = create_robust_session()\n",
        "    base_url = \"https://world.openfoodfacts.org/cgi/search.pl\"\n",
        "    products = []\n",
        "    page = 1\n",
        "\n",
        "    while len(products) < max_products:\n",
        "        params = {\n",
        "            'action': 'process',\n",
        "            'tagtype_0': 'categories',\n",
        "            'tag_contains_0': 'contains',\n",
        "            'tag_0': category,\n",
        "            'page_size': page_size,\n",
        "            'page': page,\n",
        "            'json': 1\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            print(f\"   Page {page}...\", end=\" \")\n",
        "            response = session.get(\n",
        "                base_url,\n",
        "                params=params,\n",
        "                timeout=60,\n",
        "                headers={'User-Agent': 'DietCheck-Research/1.0'}\n",
        "            )\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                page_products = data.get('products', [])\n",
        "                if not page_products:\n",
        "                    print(\"done\")\n",
        "                    break\n",
        "                products.extend(page_products)\n",
        "                print(f\"‚úì ({len(products)} total)\")\n",
        "                if len(products) >= max_products:\n",
        "                    break\n",
        "                page += 1\n",
        "                time.sleep(2)\n",
        "            else:\n",
        "                print(f\"‚úó HTTP {response.status_code}\")\n",
        "                break\n",
        "        except requests.exceptions.Timeout:\n",
        "            print(f\"‚úó timeout\")\n",
        "            time.sleep(5)\n",
        "            continue\n",
        "        except requests.exceptions.RequestException:\n",
        "            print(f\"‚úó error\")\n",
        "            break\n",
        "\n",
        "    return products[:max_products]\n",
        "\n",
        "print(\"‚úÖ Collection functions ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IjL6EdxUpwN",
        "outputId": "46d63c0b-89ef-4af7-b21f-f1ba523837ec"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Collection functions ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Product Collection\n",
        "\n",
        "Fetches products from 17 categories targeting 300+ total products."
      ],
      "metadata": {
        "id": "sv41XhEZVodu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CATEGORIES = {\n",
        "    'breakfast-cereals': 25,\n",
        "    'soups': 25,\n",
        "    'protein-products': 25,\n",
        "    'snacks': 25,\n",
        "    'beverages': 25,\n",
        "    'frozen-meals': 25,\n",
        "    'dairy-alternatives': 15,\n",
        "    'condiments': 15,\n",
        "    'yogurts': 25,\n",
        "    'cheeses': 25,\n",
        "    'breads': 25,\n",
        "    'pasta': 20,\n",
        "    'plant-based-foods': 25,\n",
        "    'canned-foods': 20,\n",
        "    'sauces': 20,\n",
        "    'spreads': 15\n",
        "}\n",
        "\n",
        "all_products = []\n",
        "print(f\" Starting collection from {len(CATEGORIES)} categories\\n\")\n",
        "\n",
        "for category, target in CATEGORIES.items():\n",
        "    print(f\"üì¶ {category} (target: {target})\")\n",
        "    products = fetch_products_from_category(category=category, page_size=15, max_products=target)\n",
        "    all_products.extend(products)\n",
        "    print(f\"    {len(products)} collected | Total: {len(all_products)}\\n\")\n",
        "    time.sleep(3)\n",
        "\n",
        "print(f\" Collection complete: {len(all_products)} products\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ujKr8BSWab6",
        "outputId": "50468e17-c38a-4414-b3af-2078c356726f"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Starting collection from 16 categories\n",
            "\n",
            "üì¶ breakfast-cereals (target: 25)\n",
            "   Page 1... ‚úì (15 total)\n",
            "   Page 2... ‚úì (30 total)\n",
            "    25 collected | Total: 25\n",
            "\n",
            "üì¶ soups (target: 25)\n",
            "   Page 1... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='world.openfoodfacts.org', port=443): Read timed out. (read timeout=60)\")': /cgi/search.pl?action=process&tagtype_0=categories&tag_contains_0=contains&tag_0=soups&page_size=15&page=1&json=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì (15 total)\n",
            "   Page 2... ‚úì (30 total)\n",
            "    25 collected | Total: 50\n",
            "\n",
            "üì¶ protein-products (target: 25)\n",
            "   Page 1... ‚úó error\n",
            "    0 collected | Total: 50\n",
            "\n",
            "üì¶ snacks (target: 25)\n",
            "   Page 1... ‚úì (15 total)\n",
            "   Page 2... ‚úì (30 total)\n",
            "    25 collected | Total: 75\n",
            "\n",
            "üì¶ beverages (target: 25)\n",
            "   Page 1... ‚úì (15 total)\n",
            "   Page 2... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='world.openfoodfacts.org', port=443): Read timed out. (read timeout=60)\")': /cgi/search.pl?action=process&tagtype_0=categories&tag_contains_0=contains&tag_0=beverages&page_size=15&page=2&json=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì (30 total)\n",
            "    25 collected | Total: 100\n",
            "\n",
            "üì¶ frozen-meals (target: 25)\n",
            "   Page 1... ‚úì (15 total)\n",
            "   Page 2... ‚úì (30 total)\n",
            "    25 collected | Total: 125\n",
            "\n",
            "üì¶ dairy-alternatives (target: 15)\n",
            "   Page 1... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='world.openfoodfacts.org', port=443): Read timed out. (read timeout=60)\")': /cgi/search.pl?action=process&tagtype_0=categories&tag_contains_0=contains&tag_0=dairy-alternatives&page_size=15&page=1&json=1\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='world.openfoodfacts.org', port=443): Read timed out. (read timeout=60)\")': /cgi/search.pl?action=process&tagtype_0=categories&tag_contains_0=contains&tag_0=dairy-alternatives&page_size=15&page=1&json=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úó error\n",
            "    0 collected | Total: 125\n",
            "\n",
            "üì¶ condiments (target: 15)\n",
            "   Page 1... ‚úì (15 total)\n",
            "    15 collected | Total: 140\n",
            "\n",
            "üì¶ yogurts (target: 25)\n",
            "   Page 1... ‚úì (15 total)\n",
            "   Page 2... ‚úì (30 total)\n",
            "    25 collected | Total: 165\n",
            "\n",
            "üì¶ cheeses (target: 25)\n",
            "   Page 1... ‚úì (15 total)\n",
            "   Page 2... ‚úì (30 total)\n",
            "    25 collected | Total: 190\n",
            "\n",
            "üì¶ breads (target: 25)\n",
            "   Page 1... ‚úì (15 total)\n",
            "   Page 2... ‚úì (30 total)\n",
            "    25 collected | Total: 215\n",
            "\n",
            "üì¶ pasta (target: 20)\n",
            "   Page 1... ‚úì (15 total)\n",
            "   Page 2... ‚úì (30 total)\n",
            "    20 collected | Total: 235\n",
            "\n",
            "üì¶ plant-based-foods (target: 25)\n",
            "   Page 1... ‚úì (15 total)\n",
            "   Page 2... ‚úì (30 total)\n",
            "    25 collected | Total: 260\n",
            "\n",
            "üì¶ canned-foods (target: 20)\n",
            "   Page 1... ‚úì (15 total)\n",
            "   Page 2... ‚úì (30 total)\n",
            "    20 collected | Total: 280\n",
            "\n",
            "üì¶ sauces (target: 20)\n",
            "   Page 1... ‚úì (15 total)\n",
            "   Page 2... ‚úì (30 total)\n",
            "    20 collected | Total: 300\n",
            "\n",
            "üì¶ spreads (target: 15)\n",
            "   Page 1... ‚úì (15 total)\n",
            "    15 collected | Total: 315\n",
            "\n",
            " Collection complete: 315 products\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Deduplicate Products\n",
        "\n",
        "Removes duplicate products by barcode."
      ],
      "metadata": {
        "id": "VgQntMGl_NQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seen_codes = set()\n",
        "unique_products = []\n",
        "\n",
        "for product in all_products:\n",
        "    code = product.get('code', product.get('_id', ''))\n",
        "    if code and code not in seen_codes:\n",
        "        seen_codes.add(code)\n",
        "        unique_products.append(product)\n",
        "\n",
        "all_products = unique_products\n",
        "\n",
        "print(f\"üìä After deduplication: {len(all_products)} unique products\")"
      ],
      "metadata": {
        "id": "RwKQjk1L_OG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Extract and Clean Product Data\n",
        "\n",
        "Extracts nutritional features, calculates derived metrics, and validates data quality."
      ],
      "metadata": {
        "id": "umnGe6-k_SWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_serving_size(serving_size_raw):\n",
        "    \"\"\"\n",
        "    Parse a free-text serving size string into approximate grams.\n",
        "\n",
        "    Strategy:\n",
        "    - Use the first numeric quantity in the string (e.g., \"30 g (1/2 cup)\" -> 30).\n",
        "    - Interpret units containing 'g' or 'gram' as grams.\n",
        "    - Interpret 'ml' as grams assuming density ~1 g/mL.\n",
        "    - If parsing fails or is clearly unreasonable, default to 100 g.\n",
        "\n",
        "    This avoids concatenating all digits (e.g., \"30 g (1/2 cup)\" -> \"3012\") which\n",
        "    badly inflates serving size and corrupts per-serving nutrition.\n",
        "    \"\"\"\n",
        "    if not serving_size_raw:\n",
        "        return 100.0\n",
        "\n",
        "    s = str(serving_size_raw)\n",
        "    match = re.search(r'(\\d+(\\.\\d+)?)', s)\n",
        "    if not match:\n",
        "        return 100.0\n",
        "\n",
        "    value = float(match.group(1))\n",
        "    s_lower = s.lower()\n",
        "\n",
        "    # Basic unit handling\n",
        "    if 'g' in s_lower or 'gram' in s_lower:\n",
        "        serving_g = value\n",
        "    elif 'ml' in s_lower:\n",
        "        # Approximate 1 mL ~ 1 g for most liquids\n",
        "        serving_g = value\n",
        "    else:\n",
        "        # Unknown unit: treat the numeric as grams but guard against absurd values\n",
        "        serving_g = value\n",
        "\n",
        "    # Guardrails against pathological values\n",
        "    if serving_g <= 0:\n",
        "        return 100.0\n",
        "    if serving_g > 1000:  # e.g., \"100 g x 12\" kind of strings\n",
        "        return 100.0\n",
        "\n",
        "    return serving_g\n",
        "\n",
        "\n",
        "def extract_product_features(product):\n",
        "    \"\"\"\n",
        "    Extract nutritional features from API response with validation.\n",
        "\n",
        "    Calculations:\n",
        "    - Net carbs = total_carbs - fiber - sugar alcohols (polyols) where available\n",
        "      (for ketogenic classification).\n",
        "    - Serving size normalization from 100 g values.\n",
        "    - Sodium conversion from g to mg.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        nutriments = product.get('nutriments', {}) or {}\n",
        "\n",
        "        # Extract base nutrition per 100 g\n",
        "        carbs_100g = nutriments.get('carbohydrates_100g', 0) or 0\n",
        "        fiber_100g = nutriments.get('fiber_100g', 0) or 0\n",
        "        polyols_100g = nutriments.get('polyols_100g', 0) or 0\n",
        "        net_carbs_100g = max(0, carbs_100g - fiber_100g - polyols_100g)\n",
        "\n",
        "        # Get serving size (default 100 g if not specified or messy)\n",
        "        serving_size_raw = product.get('serving_size', '100g')\n",
        "        serving_g = parse_serving_size(serving_size_raw)\n",
        "\n",
        "        # Calculate per-serving values\n",
        "        multiplier = serving_g / 100.0\n",
        "\n",
        "        return {\n",
        "            'product_id': product.get('code', ''),\n",
        "            'name': product.get('product_name', ''),\n",
        "            'brand': product.get('brands', ''),\n",
        "            'category': product.get('categories_tags', [''])[0]\n",
        "            if product.get('categories_tags') else '',\n",
        "            'ingredients': product.get('ingredients_text', ''),\n",
        "            'serving_size_g': serving_g,\n",
        "\n",
        "            # Per 100 g values\n",
        "            'energy_100g': nutriments.get('energy-kcal_100g', 0) or 0,\n",
        "            'fat_100g': nutriments.get('fat_100g', 0) or 0,\n",
        "            'saturated_fat_100g': nutriments.get('saturated-fat_100g', 0) or 0,\n",
        "            'carbs_100g': carbs_100g,\n",
        "            'fiber_100g': fiber_100g,\n",
        "            'sugars_100g': nutriments.get('sugars_100g', 0) or 0,\n",
        "            'protein_100g': nutriments.get('proteins_100g', 0) or 0,\n",
        "            'sodium_100g': (nutriments.get('sodium_100g', 0) or 0) * 1000,  # g ‚Üí mg\n",
        "            'net_carbs_100g': net_carbs_100g,\n",
        "            'polyols_100g': polyols_100g,\n",
        "\n",
        "            # Per serving values\n",
        "            'energy_per_serving': (nutriments.get('energy-kcal_100g', 0) or 0) * multiplier,\n",
        "            'fat_per_serving': (nutriments.get('fat_100g', 0) or 0) * multiplier,\n",
        "            'saturated_fat_per_serving': (nutriments.get('saturated-fat_100g', 0) or 0) * multiplier,\n",
        "            'carbs_per_serving': carbs_100g * multiplier,\n",
        "            'fiber_per_serving': fiber_100g * multiplier,\n",
        "            'sugars_per_serving': (nutriments.get('sugars_100g', 0) or 0) * multiplier,\n",
        "            'protein_per_serving': (nutriments.get('proteins_100g', 0) or 0) * multiplier,\n",
        "            'sodium_per_serving': (nutriments.get('sodium_100g', 0) or 0) * 1000 * multiplier,\n",
        "            'net_carbs_per_serving': net_carbs_100g * multiplier,\n",
        "            'polyols_per_serving': polyols_100g * multiplier,\n",
        "        }\n",
        "    except Exception:\n",
        "        # If anything goes wrong for this product, skip it\n",
        "        return None\n",
        "\n",
        "\n",
        "# Extract features from all products\n",
        "products_data = []\n",
        "for product in all_products:\n",
        "    features = extract_product_features(product)\n",
        "    if features:\n",
        "        products_data.append(features)\n",
        "\n",
        "df = pd.DataFrame(products_data)\n",
        "\n",
        "print(f\"‚úÖ Extracted features from {len(df)} products\")\n",
        "print(f\"üìä Features per product: {len(df.columns)}\")\n"
      ],
      "metadata": {
        "id": "kCuSn9h6_V75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Data Quality Validation\n",
        "\n",
        "Validates completeness and removes products with missing critical fields."
      ],
      "metadata": {
        "id": "1EQhz0pI_asl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required fields for dietary classification\n",
        "REQUIRED_FIELDS = [\n",
        "    'ingredients',\n",
        "    'protein_per_serving',\n",
        "    'sodium_per_serving',\n",
        "    'fat_per_serving',\n",
        "    'net_carbs_per_serving'\n",
        "]\n",
        "\n",
        "# Count missing values before cleaning\n",
        "print(\"üìä Data Quality Report (Before Cleaning):\")\n",
        "print(f\"   Total products: {len(df)}\")\n",
        "print(f\"   Missing ingredients: {df['ingredients'].isna().sum()}\")\n",
        "print(f\"   Empty ingredients: {(df['ingredients'] == '').sum()}\")\n",
        "print(f\"   Missing nutrition data: {df[REQUIRED_FIELDS[1:]].isna().any(axis=1).sum()}\")\n",
        "\n",
        "# Remove products with missing critical data\n",
        "initial_count = len(df)\n",
        "\n",
        "df = df[df['ingredients'].notna() & (df['ingredients'] != '')]\n",
        "df = df[df[REQUIRED_FIELDS[1:]].notna().all(axis=1)]\n",
        "df = df[df['product_id'] != '']\n",
        "\n",
        "# Remove duplicates by product_id\n",
        "df = df.drop_duplicates(subset=['product_id'])\n",
        "\n",
        "# Reset index\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nüìä Data Quality Report (After Cleaning):\")\n",
        "print(f\"   Products retained: {len(df)}\")\n",
        "print(f\"   Products removed: {initial_count - len(df)}\")\n",
        "print(f\"   Retention rate: {len(df)/initial_count*100:.1f}%\")"
      ],
      "metadata": {
        "id": "tYBIMqrL_bpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Apply FDA Dietary Labels\n",
        "\n",
        "Applies FDA regulatory thresholds for dietary classification.\n",
        "\n",
        "**Thresholds:**\n",
        "- Keto: ‚â§5g net carbs/serving\n",
        "- High Protein: ‚â•10g protein/serving (20% DV)\n",
        "- Low Sodium: ‚â§140mg sodium/serving\n",
        "- Low Fat: ‚â§3g fat/serving"
      ],
      "metadata": {
        "id": "iYIB1X-J_drP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FDA_THRESHOLDS = {\n",
        "    'keto_compliant': {\n",
        "        'feature': 'net_carbs_per_serving',\n",
        "        'threshold': 5.0,\n",
        "        'operator': '<=',\n",
        "        'source': 'Ketogenic diet standard'\n",
        "    },\n",
        "    'high_protein': {\n",
        "        'feature': 'protein_per_serving',\n",
        "        'threshold': 10.0,\n",
        "        'operator': '>=',\n",
        "        'source': 'FDA 21 CFR ¬ß101.54(b)'\n",
        "    },\n",
        "    'low_sodium': {\n",
        "        'feature': 'sodium_per_serving',\n",
        "        'threshold': 140.0,\n",
        "        'operator': '<=',\n",
        "        'source': 'FDA 21 CFR ¬ß101.61(b)(4)'\n",
        "    },\n",
        "    'low_fat': {\n",
        "        'feature': 'fat_per_serving',\n",
        "        'threshold': 3.0,\n",
        "        'operator': '<=',\n",
        "        'source': 'FDA 21 CFR ¬ß101.62(b)(2)'\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "def apply_dietary_labels(row, thresholds, conservative_margin=0.10):\n",
        "    \"\"\"\n",
        "    Apply FDA threshold-based classification with a conservative band.\n",
        "\n",
        "    Rule:\n",
        "    - Start from the usual rule:\n",
        "        - '<=' labels compliant if value <= threshold.\n",
        "        - '>=' labels compliant if value >= threshold.\n",
        "    - Then, if the value is within ¬±conservative_margin of the threshold\n",
        "      (relative difference), force label = 0 (non-compliant) to avoid\n",
        "      optimistic labeling around the boundary.\n",
        "    \"\"\"\n",
        "    labels = {}\n",
        "    for label, config in thresholds.items():\n",
        "        feature = config['feature']\n",
        "        threshold = config['threshold']\n",
        "        operator = config['operator']\n",
        "\n",
        "        value = row.get(feature, np.nan)\n",
        "\n",
        "        # Missing ‚Üí non-compliant\n",
        "        if pd.isna(value):\n",
        "            labels[label] = 0\n",
        "            continue\n",
        "\n",
        "        # Base decision\n",
        "        if operator == '<=':\n",
        "            compliant = value <= threshold\n",
        "        else:  # '>='\n",
        "            compliant = value >= threshold\n",
        "\n",
        "        # Apply conservative band around the threshold\n",
        "        if compliant and threshold > 0:\n",
        "            rel_diff = abs(value - threshold) / threshold\n",
        "            if rel_diff <= conservative_margin:\n",
        "                compliant = False\n",
        "\n",
        "        labels[label] = int(compliant)\n",
        "\n",
        "    return pd.Series(labels)\n",
        "\n",
        "\n",
        "# Apply labels\n",
        "label_df = df.apply(lambda row: apply_dietary_labels(row, FDA_THRESHOLDS), axis=1)\n",
        "df = pd.concat([df, label_df], axis=1)\n",
        "\n",
        "print(\"‚úÖ FDA labels applied\")\n",
        "print(\"\\nüìä Label Distribution:\")\n",
        "for label in FDA_THRESHOLDS.keys():\n",
        "    count = df[label].sum()\n",
        "    pct = (count / len(df)) * 100\n",
        "    print(f\"   {label}: {count}/{len(df)} ({pct:.1f}%)\")\n"
      ],
      "metadata": {
        "id": "TlLfWxC__gCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Dataset Statistics and Visualization\n",
        "\n",
        "Analyzes label distribution and nutritional feature ranges."
      ],
      "metadata": {
        "id": "d30lAHl1_jQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Label distribution\n",
        "label_counts = [df[label].sum() for label in FDA_THRESHOLDS.keys()]\n",
        "axes[0, 0].bar(FDA_THRESHOLDS.keys(), label_counts, color='steelblue')\n",
        "axes[0, 0].set_title('FDA Label Distribution')\n",
        "axes[0, 0].set_ylabel('Count')\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Nutritional features distribution\n",
        "nutrients = ['protein_per_serving', 'sodium_per_serving', 'fat_per_serving', 'net_carbs_per_serving']\n",
        "positions = [(0, 1), (1, 0), (1, 1)]\n",
        "\n",
        "for idx, nutrient in enumerate(nutrients[1:]):\n",
        "    row, col = positions[idx]\n",
        "    axes[row, col].hist(df[nutrient], bins=30, color='coral', edgecolor='black')\n",
        "    axes[row, col].set_title(f'{nutrient.replace(\"_\", \" \").title()}')\n",
        "    axes[row, col].set_xlabel('Value')\n",
        "    axes[row, col].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(RESULTS_DIR, 'dataset_statistics.png'), dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"‚úÖ Visualization saved to {RESULTS_DIR}/dataset_statistics.png\")"
      ],
      "metadata": {
        "id": "QFJlJX_x_kwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Train-Test Split\n",
        "\n",
        "Creates stratified split attempting to balance label distribution.\n",
        "Falls back to random split if stratification fails due to rare label combinations."
      ],
      "metadata": {
        "id": "IzhEsUrz_oUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LABEL_COLS = ['keto_compliant', 'high_protein', 'low_sodium', 'low_fat']\n",
        "\n",
        "# Create multi-label string for stratification attempt\n",
        "df['label_combination'] = df[LABEL_COLS].apply(lambda x: '_'.join(x.astype(str)), axis=1)\n",
        "\n",
        "# Check combination frequencies\n",
        "combination_counts = df['label_combination'].value_counts()\n",
        "print(\"üìä Label Combination Frequencies:\")\n",
        "print(f\"   Unique combinations: {len(combination_counts)}\")\n",
        "print(f\"   Singleton combinations: {(combination_counts == 1).sum()}\")\n",
        "\n",
        "# Attempt stratified split\n",
        "try:\n",
        "    train_df, test_df = train_test_split(\n",
        "        df,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=df['label_combination']\n",
        "    )\n",
        "    print(\"\\n‚úÖ Stratified split successful\")\n",
        "except ValueError:\n",
        "    print(\"\\n‚ö†Ô∏è  Stratification failed (rare label combinations)\")\n",
        "    print(\"   Using random split instead\")\n",
        "    train_df, test_df = train_test_split(\n",
        "        df,\n",
        "        test_size=0.2,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "# Drop temporary column\n",
        "train_df = train_df.drop('label_combination', axis=1)\n",
        "test_df = test_df.drop('label_combination', axis=1)\n",
        "\n",
        "print(f\"\\nüìä Split Summary:\")\n",
        "print(f\"   Train set: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)\")\n",
        "print(f\"   Test set: {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)\")\n",
        "\n",
        "print(\"\\nüìä Train Set Label Distribution:\")\n",
        "for label in LABEL_COLS:\n",
        "    count = train_df[label].sum()\n",
        "    pct = (count / len(train_df)) * 100\n",
        "    print(f\"   {label}: {count}/{len(train_df)} ({pct:.1f}%)\")\n",
        "\n",
        "print(\"\\nüìä Test Set Label Distribution:\")\n",
        "for label in LABEL_COLS:\n",
        "    count = test_df[label].sum()\n",
        "    pct = (count / len(test_df)) * 100\n",
        "    print(f\"   {label}: {count}/{len(test_df)} ({pct:.1f}%)\")"
      ],
      "metadata": {
        "id": "uR8_4AC9_pty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Save Datasets\n",
        "\n",
        "Saves full dataset and train/test splits to CSV files."
      ],
      "metadata": {
        "id": "f3phLRi3_rgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save datasets\n",
        "products_path = os.path.join(DATA_DIR, 'products.csv')\n",
        "train_path = os.path.join(DATA_DIR, 'train.csv')\n",
        "test_path = os.path.join(DATA_DIR, 'test.csv')\n",
        "\n",
        "df.drop('label_combination', axis=1, errors='ignore').to_csv(products_path, index=False)\n",
        "train_df.to_csv(train_path, index=False)\n",
        "test_df.to_csv(test_path, index=False)\n",
        "\n",
        "print(\"‚úÖ Datasets saved:\")\n",
        "print(f\"   {products_path}\")\n",
        "print(f\"   {train_path}\")\n",
        "print(f\"   {test_path}\")\n",
        "\n",
        "print(f\"\\nüìä Final Dataset Summary:\")\n",
        "print(f\"   Total products: {len(df)}\")\n",
        "print(f\"   Features: {len(df.columns)}\")\n",
        "print(f\"   Train samples: {len(train_df)}\")\n",
        "print(f\"   Test samples: {len(test_df)}\")"
      ],
      "metadata": {
        "id": "BDZqKWzl_tn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. Dataset Metadata and Reproducibility\n",
        "\n",
        "Documents dataset characteristics for reproducibility and reporting."
      ],
      "metadata": {
        "id": "uar_FCa3_v_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = {\n",
        "    'dataset_size': len(df),\n",
        "    'train_size': len(train_df),\n",
        "    'test_size': len(test_df),\n",
        "    'num_features': len(df.columns),\n",
        "    'label_distribution': {\n",
        "        label: {\n",
        "            'total': int(df[label].sum()),\n",
        "            'percentage': float((df[label].sum() / len(df)) * 100),\n",
        "            'train': int(train_df[label].sum()),\n",
        "            'test': int(test_df[label].sum())\n",
        "        }\n",
        "        for label in LABEL_COLS\n",
        "    },\n",
        "    'fda_thresholds': FDA_THRESHOLDS,\n",
        "    'collection_date': pd.Timestamp.now().strftime('%Y-%m-%d'),\n",
        "    'categories_collected': list(CATEGORIES.keys()),\n",
        "    'random_seed': 42\n",
        "}\n",
        "\n",
        "metadata_path = os.path.join(DATA_DIR, 'dataset_metadata.json')\n",
        "with open(metadata_path, 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Metadata saved\")\n",
        "print(f\"\\nüìÑ Dataset Metadata:\")\n",
        "print(json.dumps(metadata, indent=2))"
      ],
      "metadata": {
        "id": "qsY1bo-E_xTk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}